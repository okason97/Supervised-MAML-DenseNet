{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from densenet import densenet_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From <ipython-input-2-0dcb167e88d7>:14: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# data\n",
    "rotation_range = 20\n",
    "width_shift_range = 0.2\n",
    "height_shift_range = 0.2\n",
    "horizontal_flip = True\n",
    "vertical_flip = True\n",
    "shear_range = 0\n",
    "zoom_range = 0.5\n",
    "size = (32,32)\n",
    "\n",
    "# model\n",
    "nb_filter = 64\n",
    "growth_rate = 64\n",
    "nb_layers = [6, 12, 16]\n",
    "reduction = 0.5\n",
    "\n",
    "# training\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "max_patience = 100\n",
    "batch_size = 256\n",
    "train_epochs = 3\n",
    "\n",
    "# log\n",
    "log_freq = 1\n",
    "models_directory = 'results/models/'\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "identifier = \"{}-growth-{}-densenet\".format(\n",
    "    '-'.join([str(i) for i in nb_layers]),\n",
    "    growth_rate) + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tf.data.Dataset\n",
    "ds_train = tfds.load('cifar100', split='train', shuffle_files=True, batch_size=-1)\n",
    "train_np_ds = tfds.as_numpy(ds_train)\n",
    "ds_test = tfds.load('cifar100', split='test', shuffle_files=False, batch_size=-1)\n",
    "test_np_ds = tfds.as_numpy(ds_test)\n",
    "\n",
    "x_train, y_train = train_np_ds[\"image\"], train_np_ds[\"label\"]\n",
    "x_test, y_test = test_np_ds[\"image\"], test_np_ds[\"label\"]\n",
    "\n",
    "# shuffle the meta train images maintaining the same label order\n",
    "index_sets = [np.argwhere(i==y_train) for i in np.unique(y_train)]\n",
    "x_meta_train = np.copy(x_train)\n",
    "for class_indexes in index_sets:\n",
    "    shuffled_class_indexes = np.copy(class_indexes)\n",
    "    np.random.shuffle(shuffled_class_indexes)\n",
    "    for i in range(len(class_indexes)):\n",
    "        x_meta_train[class_indexes[i]] = x_train[shuffled_class_indexes[i]]\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "\n",
    "info = tfds.builder('cifar100').info\n",
    "n_classes = info.features['label'].num_classes\n",
    "img_shape = info.features['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=rotation_range,\n",
    "    width_shift_range=width_shift_range,\n",
    "    height_shift_range=height_shift_range,\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    vertical_flip = vertical_flip,\n",
    "    shear_range=shear_range,\n",
    "    zoom_range=zoom_range,\n",
    "    fill_mode='constant',\n",
    "    cval=0,\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    ")\n",
    "\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# create data generators\n",
    "train_gen =  datagen.flow(x_train, y_train, batch_size=batch_size, seed=42)\n",
    "meta_train_gen =  datagen.flow(x_meta_train, y_train, batch_size=batch_size, seed=42)\n",
    "test_gen = test_datagen.flow(x_test, y_test , batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "batches = 0\n",
    "for images, labels in train_gen:\n",
    "    batches += 1\n",
    "    all_labels = np.append(all_labels, labels)    \n",
    "    if batches >= train_size / batch_size:\n",
    "        # we need to break the loop by hand because\n",
    "        # the generator loops indefinitely\n",
    "        break\n",
    "all_meta_labels = []\n",
    "batches = 0\n",
    "for images, labels in meta_train_gen:\n",
    "    batches += 1\n",
    "    all_meta_labels = np.append(all_meta_labels, labels)    \n",
    "    if batches >= train_size / batch_size:\n",
    "        # we need to break the loop by hand because\n",
    "        # the generator loops indefinitely\n",
    "        break\n",
    "print(all_labels == all_meta_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_gen:\n",
    "    plt.imshow(images[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet_model(classes=n_classes, nb_filter=nb_filter, shape=img_shape, growth_rate=growth_rate, nb_layers=nb_layers, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "meta_optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def meta_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary writers\n",
    "train_summary_writer = tf.summary.create_file_writer('results/summaries/train/' + identifier)\n",
    "test_summary_writer = tf.summary.create_file_writer('results/summaries/test/' + identifier)\n",
    "\n",
    "min_loss = 100\n",
    "min_loss_acc = 0\n",
    "patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 0, Train Loss: 4.791623592376709, Train Acc:9.314499855041504, Test Loss: 3.6917662620544434, Test Acc: 13.809999465942383, Time: 362.89227175712585 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 1, Train Loss: 3.9383606910705566, Train Acc:17.04450035095215, Test Loss: 3.4201340675354004, Test Acc: 18.15999984741211, Time: 335.12327241897583 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 2, Train Loss: 3.348360538482666, Train Acc:24.04199981689453, Test Loss: 3.2014572620391846, Test Acc: 22.90999984741211, Time: 334.28536462783813 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 3, Train Loss: 3.1191158294677734, Train Acc:28.519498825073242, Test Loss: 3.2106246948242188, Test Acc: 21.989999771118164, Time: 333.8260567188263 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 4, Train Loss: 2.8037197589874268, Train Acc:33.75299835205078, Test Loss: 3.192918539047241, Test Acc: 22.110000610351562, Time: 334.3367500305176 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 5, Train Loss: 2.908780813217163, Train Acc:34.47200012207031, Test Loss: 3.200261354446411, Test Acc: 23.399999618530273, Time: 333.9000475406647 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 6, Train Loss: 1.9129650592803955, Train Acc:47.98749923706055, Test Loss: 3.3043861389160156, Test Acc: 23.970001220703125, Time: 334.1855170726776 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 7, Train Loss: 2.417558431625366, Train Acc:43.36899948120117, Test Loss: 3.401029586791992, Test Acc: 23.760000228881836, Time: 333.41637563705444 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 8, Train Loss: 1.8043910264968872, Train Acc:51.20750045776367, Test Loss: 2.98867130279541, Test Acc: 28.880001068115234, Time: 333.28443217277527 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 9, Train Loss: 1.9530506134033203, Train Acc:49.67850112915039, Test Loss: 3.370318651199341, Test Acc: 26.39000129699707, Time: 333.97896027565 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 10, Train Loss: 1.8121658563613892, Train Acc:53.2869987487793, Test Loss: 3.3521366119384766, Test Acc: 26.280000686645508, Time: 333.4363992214203 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 11, Train Loss: 1.3646998405456543, Train Acc:61.55949783325195, Test Loss: 3.8502144813537598, Test Acc: 23.93000030517578, Time: 333.1019694805145 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 12, Train Loss: 1.5670603513717651, Train Acc:60.778499603271484, Test Loss: 4.116145133972168, Test Acc: 24.09000015258789, Time: 333.8039376735687 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 13, Train Loss: 1.0492267608642578, Train Acc:69.52300262451172, Test Loss: 3.973017454147339, Test Acc: 25.760000228881836, Time: 333.6344680786133 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 14, Train Loss: 1.1547096967697144, Train Acc:68.60600280761719, Test Loss: 4.369086265563965, Test Acc: 22.670000076293945, Time: 332.6450545787811 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 15, Train Loss: 1.0540399551391602, Train Acc:71.28499603271484, Test Loss: 4.113917827606201, Test Acc: 26.750001907348633, Time: 333.29758739471436 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 16, Train Loss: 0.6482258439064026, Train Acc:80.56600189208984, Test Loss: 4.490278244018555, Test Acc: 25.84000015258789, Time: 333.64458441734314 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 17, Train Loss: 1.0103511810302734, Train Acc:73.63200378417969, Test Loss: 4.377961158752441, Test Acc: 26.660001754760742, Time: 333.1315014362335 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 18, Train Loss: 0.7914282083511353, Train Acc:78.0040054321289, Test Loss: 4.591595649719238, Test Acc: 25.299999237060547, Time: 333.80111289024353 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 19, Train Loss: 0.4728093445301056, Train Acc:85.63999938964844, Test Loss: 4.57003116607666, Test Acc: 27.34000015258789, Time: 333.9077892303467 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 20, Train Loss: 0.5810626745223999, Train Acc:82.93899536132812, Test Loss: 4.7267255783081055, Test Acc: 27.919998168945312, Time: 333.23695182800293 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 21, Train Loss: 0.8707984089851379, Train Acc:78.52249908447266, Test Loss: 5.556467056274414, Test Acc: 23.3700008392334, Time: 333.4781892299652 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 22, Train Loss: 0.3526451587677002, Train Acc:89.27400207519531, Test Loss: 5.052536964416504, Test Acc: 26.05000114440918, Time: 333.79738879203796 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 23, Train Loss: 0.6380473375320435, Train Acc:83.23049926757812, Test Loss: 5.385255813598633, Test Acc: 24.690000534057617, Time: 333.27865767478943 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 24, Train Loss: 0.3239910900592804, Train Acc:90.04150390625, Test Loss: 5.436484336853027, Test Acc: 25.0, Time: 333.72627544403076 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 25, Train Loss: 0.6244600415229797, Train Acc:83.4739990234375, Test Loss: 5.530515193939209, Test Acc: 26.239999771118164, Time: 333.254207611084 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 26, Train Loss: 0.6798879504203796, Train Acc:82.98750305175781, Test Loss: 5.504976749420166, Test Acc: 24.42999839782715, Time: 333.5336878299713 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 27, Train Loss: 0.2863333821296692, Train Acc:91.35700225830078, Test Loss: 5.746884346008301, Test Acc: 25.310001373291016, Time: 333.76778531074524 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 28, Train Loss: 0.6283069849014282, Train Acc:84.09600067138672, Test Loss: 5.948528289794922, Test Acc: 24.110000610351562, Time: 332.9855523109436 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 29, Train Loss: 0.26157084107398987, Train Acc:92.04750061035156, Test Loss: 6.252226829528809, Test Acc: 22.989999771118164, Time: 333.28176951408386 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 30, Train Loss: 0.2534092664718628, Train Acc:92.28050231933594, Test Loss: 6.150042533874512, Test Acc: 24.850000381469727, Time: 333.5611128807068 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 31, Train Loss: 0.6809419393539429, Train Acc:83.86849975585938, Test Loss: 6.497191429138184, Test Acc: 24.349998474121094, Time: 333.1575884819031 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 32, Train Loss: 0.2332148402929306, Train Acc:92.8635025024414, Test Loss: 6.306339740753174, Test Acc: 23.5, Time: 333.671098947525 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 33, Train Loss: 0.2456647753715515, Train Acc:92.56399536132812, Test Loss: 6.256711006164551, Test Acc: 24.190000534057617, Time: 333.51991176605225 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 34, Train Loss: 0.5255506038665771, Train Acc:86.57649993896484, Test Loss: 6.693325042724609, Test Acc: 22.869998931884766, Time: 333.30977177619934 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 35, Train Loss: 0.22193609178066254, Train Acc:93.21050262451172, Test Loss: 6.446815490722656, Test Acc: 24.959999084472656, Time: 333.6000919342041 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 36, Train Loss: 0.38901880383491516, Train Acc:89.06800079345703, Test Loss: 6.351231575012207, Test Acc: 20.610000610351562, Time: 332.95969891548157 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 37, Train Loss: 0.23847056925296783, Train Acc:92.8134994506836, Test Loss: 6.270484924316406, Test Acc: 25.580001831054688, Time: 333.42607402801514 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 38, Train Loss: 0.4617496430873871, Train Acc:87.91900634765625, Test Loss: 6.343350410461426, Test Acc: 22.270000457763672, Time: 333.4742407798767 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 39, Train Loss: 0.20239195227622986, Train Acc:93.8584976196289, Test Loss: 6.475496768951416, Test Acc: 24.439998626708984, Time: 333.3116648197174 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 40, Train Loss: 0.1876891404390335, Train Acc:94.27400207519531, Test Loss: 6.8352227210998535, Test Acc: 24.42999839782715, Time: 333.7414302825928 s\n",
      "train epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 41, Train Loss: 0.3884947597980499, Train Acc:89.989501953125, Test Loss: 7.189992427825928, Test Acc: 23.059999465942383, Time: 333.294397354126 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 42, Train Loss: 0.1872801035642624, Train Acc:94.322998046875, Test Loss: 7.343153476715088, Test Acc: 23.729999542236328, Time: 333.7393891811371 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 43, Train Loss: 0.3505728542804718, Train Acc:90.3115005493164, Test Loss: 6.283110618591309, Test Acc: 23.309999465942383, Time: 333.7105498313904 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 44, Train Loss: 0.1854812502861023, Train Acc:94.43099975585938, Test Loss: 7.277207851409912, Test Acc: 23.5, Time: 333.0819134712219 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 45, Train Loss: 0.17867028713226318, Train Acc:94.61600494384766, Test Loss: 7.184941291809082, Test Acc: 23.489999771118164, Time: 333.515771150589 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 46, Train Loss: 0.386460542678833, Train Acc:90.50550079345703, Test Loss: 6.759330749511719, Test Acc: 24.469999313354492, Time: 332.8386981487274 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 47, Train Loss: 0.16317911446094513, Train Acc:95.01750183105469, Test Loss: 8.05455207824707, Test Acc: 23.68000030517578, Time: 333.34330105781555 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 48, Train Loss: 0.16705742478370667, Train Acc:94.96749877929688, Test Loss: 6.864625453948975, Test Acc: 24.010000228881836, Time: 332.8930199146271 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 49, Train Loss: 0.175301656126976, Train Acc:94.65499877929688, Test Loss: 7.545628547668457, Test Acc: 23.010000228881836, Time: 333.62674951553345 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 50, Train Loss: 0.4109366238117218, Train Acc:90.0615005493164, Test Loss: 7.747244358062744, Test Acc: 22.310001373291016, Time: 333.4878945350647 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 51, Train Loss: 0.15174660086631775, Train Acc:95.39649963378906, Test Loss: 7.639991760253906, Test Acc: 23.90999984741211, Time: 333.9532940387726 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 52, Train Loss: 0.15036003291606903, Train Acc:95.39350128173828, Test Loss: 7.14501953125, Test Acc: 25.969999313354492, Time: 333.28650522232056 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 53, Train Loss: 0.15431523323059082, Train Acc:95.35499572753906, Test Loss: 7.199221134185791, Test Acc: 23.810001373291016, Time: 333.1471092700958 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 54, Train Loss: 0.15648506581783295, Train Acc:95.30550384521484, Test Loss: 7.312587738037109, Test Acc: 24.130001068115234, Time: 332.66298389434814 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 55, Train Loss: 0.19968007504940033, Train Acc:94.15499877929688, Test Loss: 7.9208502769470215, Test Acc: 24.15999984741211, Time: 333.66574120521545 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 56, Train Loss: 0.1763538122177124, Train Acc:94.7030029296875, Test Loss: 7.9393415451049805, Test Acc: 21.44999885559082, Time: 333.3501467704773 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 57, Train Loss: 0.14496904611587524, Train Acc:95.67499542236328, Test Loss: 7.721856117248535, Test Acc: 24.25, Time: 333.3697340488434 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 58, Train Loss: 0.3214925229549408, Train Acc:92.02549743652344, Test Loss: 7.728594779968262, Test Acc: 22.979999542236328, Time: 334.6606647968292 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 59, Train Loss: 0.13507981598377228, Train Acc:95.85549926757812, Test Loss: 8.170133590698242, Test Acc: 22.770000457763672, Time: 333.46272015571594 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 60, Train Loss: 0.1351141482591629, Train Acc:95.86399841308594, Test Loss: 8.245342254638672, Test Acc: 23.630001068115234, Time: 333.48236560821533 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 61, Train Loss: 0.13918572664260864, Train Acc:95.76499938964844, Test Loss: 7.657259464263916, Test Acc: 23.09000015258789, Time: 333.8618371486664 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 62, Train Loss: 0.13671478629112244, Train Acc:95.8530044555664, Test Loss: 7.803450107574463, Test Acc: 23.450000762939453, Time: 333.07999753952026 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 63, Train Loss: 0.310459166765213, Train Acc:92.302001953125, Test Loss: 7.447813510894775, Test Acc: 24.040000915527344, Time: 332.68994879722595 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 64, Train Loss: 0.12376197427511215, Train Acc:96.2405014038086, Test Loss: 7.814047813415527, Test Acc: 22.739999771118164, Time: 332.3587157726288 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 65, Train Loss: 0.12454000860452652, Train Acc:96.19100189208984, Test Loss: 7.465058326721191, Test Acc: 25.770000457763672, Time: 333.33350825309753 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 66, Train Loss: 0.12318046391010284, Train Acc:96.2509994506836, Test Loss: 8.607534408569336, Test Acc: 22.55000114440918, Time: 333.8247127532959 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 67, Train Loss: 0.12952794134616852, Train Acc:96.0780029296875, Test Loss: 8.035143852233887, Test Acc: 23.80000114440918, Time: 333.86103868484497 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 68, Train Loss: 0.2843875586986542, Train Acc:92.8479995727539, Test Loss: 8.074016571044922, Test Acc: 24.190000534057617, Time: 334.10468554496765 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 69, Train Loss: 0.11707964539527893, Train Acc:96.4280014038086, Test Loss: 8.124077796936035, Test Acc: 22.610000610351562, Time: 332.4427990913391 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 70, Train Loss: 0.1149085983633995, Train Acc:96.46800231933594, Test Loss: 7.965797424316406, Test Acc: 24.579999923706055, Time: 333.1804220676422 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 71, Train Loss: 0.11819044500589371, Train Acc:96.4384994506836, Test Loss: 7.99337911605835, Test Acc: 24.3799991607666, Time: 333.6805648803711 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 72, Train Loss: 0.11851343512535095, Train Acc:96.40449523925781, Test Loss: 9.093183517456055, Test Acc: 19.889999389648438, Time: 332.8027241230011 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 73, Train Loss: 0.24734269082546234, Train Acc:93.72350311279297, Test Loss: 8.692930221557617, Test Acc: 21.760000228881836, Time: 333.1529140472412 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 74, Train Loss: 0.11264517903327942, Train Acc:96.60250091552734, Test Loss: 7.592381954193115, Test Acc: 24.51999855041504, Time: 333.98428893089294 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 75, Train Loss: 0.11268606036901474, Train Acc:96.57449340820312, Test Loss: 9.18187141418457, Test Acc: 21.510000228881836, Time: 333.5978400707245 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 76, Train Loss: 0.10885884612798691, Train Acc:96.72000122070312, Test Loss: 9.661138534545898, Test Acc: 21.610000610351562, Time: 333.67601108551025 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 77, Train Loss: 0.11080273985862732, Train Acc:96.65599822998047, Test Loss: 8.286222457885742, Test Acc: 24.40999984741211, Time: 333.2518880367279 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 78, Train Loss: 0.11366275697946548, Train Acc:96.5459976196289, Test Loss: 8.504777908325195, Test Acc: 22.670000076293945, Time: 333.14483428001404 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 79, Train Loss: 0.11038239300251007, Train Acc:96.65250396728516, Test Loss: 9.929754257202148, Test Acc: 18.649999618530273, Time: 333.59265661239624 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 80, Train Loss: 0.10889149457216263, Train Acc:96.68199920654297, Test Loss: 8.994937896728516, Test Acc: 21.510000228881836, Time: 333.0233964920044 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 81, Train Loss: 0.20100052654743195, Train Acc:94.53300476074219, Test Loss: 8.398717880249023, Test Acc: 22.6200008392334, Time: 333.09273052215576 s\n",
      "train epoch: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1\n",
      "train epoch: 2\n",
      "Epoch: 82, Train Loss: 0.1040768101811409, Train Acc:96.82550048828125, Test Loss: 8.20551872253418, Test Acc: 23.610000610351562, Time: 333.77868485450745 s\n",
      "train epoch: 0\n",
      "train epoch: 1\n",
      "train epoch: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-35cb20bcab94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# train on the task (one epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             x = self.image_data_generator.apply_transform(\n\u001b[0;32m--> 153\u001b[0;31m                 x.astype(self.dtype), params)\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(self, x, transform_parameters)\u001b[0m\n\u001b[1;32m    868\u001b[0m                                    \u001b[0mfill_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                                    \u001b[0mcval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                                    order=self.interpolation_order)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'channel_shift_intensity'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\u001b[0m in \u001b[0;36mapply_affine_transform\u001b[0;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             cval=cval) for x_channel in x]\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             cval=cval) for x_channel in x]\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         _nd_image.geometric_transform(filtered, None, None, matrix, offset,\n\u001b[0;32m--> 486\u001b[0;31m                                       output, order, mode, cval, None, None)\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"starting training\")\n",
    "time_record = ''\n",
    "for epoch in range(epochs):\n",
    "    time_start = time.time()\n",
    "\n",
    "    for train_epoch in range(train_epochs):\n",
    "        print(\"train epoch: \" + str(train_epoch))\n",
    "        batches = 0\n",
    "        while batches < train_size / batch_size:\n",
    "\n",
    "            batches += 1\n",
    "\n",
    "            # get the weights of the initial model that will do the meta learning\n",
    "            meta_model_weights = model.get_weights()\n",
    "\n",
    "            # train on the task (one batch)\n",
    "            images, labels = train_gen.next()\n",
    "            train_step(images, labels)\n",
    "\n",
    "            # test on the validation set the improvement achieved on one task for the meta learning\n",
    "            images, labels = meta_train_gen.next()\n",
    "            gradients = meta_step(images, labels)\n",
    "\n",
    "            # set weights of the model to the weights of the original model\n",
    "            model.set_weights(meta_model_weights)                        \n",
    "\n",
    "            # update the weights of the meta learning model using the loss obtained from testing\n",
    "            meta_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # get the weights of the initial model that will do the meta learning\n",
    "    meta_model_weights = model.get_weights()\n",
    "\n",
    "    # train on the task (one epoch)\n",
    "    batches = 0\n",
    "    for images, labels in train_gen:\n",
    "        batches += 1\n",
    "        train_step(images, labels)\n",
    "        if batches >= train_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    # test the newly trained model on the training set\n",
    "    batches = 0\n",
    "    for test_images, test_labels in test_gen:\n",
    "        test_step(test_images, test_labels)\n",
    "        batches += 1\n",
    "        if batches >= test_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "    \n",
    "    # set weights of the model to the weights of the original model\n",
    "    model.set_weights(meta_model_weights)                        \n",
    "\n",
    "    time_finish = time.time()\n",
    "    end_time = (time_finish-time_start)\n",
    "    time_record = time_record + '{:.3f} s \\n'.format(end_time)\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}, Time: {} s'.format(\n",
    "               epoch,\n",
    "               train_loss.result(),\n",
    "               train_accuracy.result()*100,\n",
    "               test_loss.result(),\n",
    "               test_accuracy.result()*100,\n",
    "               end_time))\n",
    "\n",
    "        if (test_loss.result() < min_loss):    \n",
    "            if not os.path.exists(models_directory):\n",
    "                os.makedirs(models_directory)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(models_directory + \"best{}.h5\".format(identifier))\n",
    "            min_loss = test_loss.result()\n",
    "            min_loss_acc = test_accuracy.result()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "            train_loss.reset_states()           \n",
    "            train_accuracy.reset_states()           \n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            test_loss.reset_states()           \n",
    "            test_accuracy.reset_states()   \n",
    "\n",
    "    if patience >= max_patience:\n",
    "        break\n",
    "\n",
    "with open(os.path.join('results/', identifier), \"w\") as file1:\n",
    "    file1.write(time_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
