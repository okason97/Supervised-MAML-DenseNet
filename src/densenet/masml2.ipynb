{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from densenet import densenet_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From <ipython-input-2-0dcb167e88d7>:14: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# data\n",
    "rotation_range = 20\n",
    "width_shift_range = 0.2\n",
    "height_shift_range = 0.2\n",
    "horizontal_flip = True\n",
    "vertical_flip = True\n",
    "shear_range = 0\n",
    "zoom_range = 0.5\n",
    "size = (32,32)\n",
    "\n",
    "# model\n",
    "nb_filter = 64\n",
    "growth_rate = 16\n",
    "nb_layers = [6, 12, 24, 16]\n",
    "reduction = 0.5\n",
    "\n",
    "# training\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "max_patience = 100\n",
    "batch_size = 512\n",
    "\n",
    "# log\n",
    "log_freq = 1\n",
    "models_directory = 'results/models/'\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "identifier = \"{}-growth-{}-densenet\".format(\n",
    "    '-'.join([str(i) for i in nb_layers]),\n",
    "    growth_rate) + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tf.data.Dataset\n",
    "ds_train = tfds.load('cifar100', split='train', shuffle_files=True, batch_size=-1)\n",
    "train_np_ds = tfds.as_numpy(ds_train)\n",
    "ds_test = tfds.load('cifar100', split='test', shuffle_files=False, batch_size=-1)\n",
    "test_np_ds = tfds.as_numpy(ds_test)\n",
    "\n",
    "x_train, y_train = train_np_ds[\"image\"], train_np_ds[\"label\"]\n",
    "x_test, y_test = test_np_ds[\"image\"], test_np_ds[\"label\"]\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "task_train_size = 1\n",
    "meta_train_size = 1\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "\n",
    "info = tfds.builder('cifar100').info\n",
    "n_classes = info.features['label'].num_classes\n",
    "img_shape = info.features['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=rotation_range,\n",
    "    width_shift_range=width_shift_range,\n",
    "    height_shift_range=height_shift_range,\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    vertical_flip = vertical_flip,\n",
    "    shear_range=shear_range,\n",
    "    zoom_range=zoom_range,\n",
    "    fill_mode='constant',\n",
    "    cval=0,\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    ")\n",
    "\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# create data generators\n",
    "train_gen =  datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "test_gen = test_datagen.flow(x_test, y_test , batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [ 2.6459384  2.7845297  2.6185014]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " [[ 2.6644511  2.8040118  2.636822 ]\n",
      "  [ 2.6536202  2.7926137  2.6261034]\n",
      "  [ 2.6400557  2.7737796  2.6083925]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " [[ 2.6535938  2.7925858  2.6260774]\n",
      "  [ 2.6438718  2.7823546  2.616456 ]\n",
      "  [ 2.5101938  2.6385581  2.4812336]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " [[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " [[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_gen:\n",
    "    print(images[0])\n",
    "    plt.imshow(images[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet_model(classes=n_classes, nb_filter=nb_filter, shape=img_shape, growth_rate=growth_rate, nb_layers=nb_layers, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "meta_optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def meta_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary writers\n",
    "train_summary_writer = tf.summary.create_file_writer('results/summaries/train/' + identifier)\n",
    "test_summary_writer = tf.summary.create_file_writer('results/summaries/test/' + identifier)\n",
    "\n",
    "min_loss = 100\n",
    "min_loss_acc = 0\n",
    "patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Epoch: 0, Train Loss: 4.450913429260254, Train Acc:3.929832935333252, Test Loss: 4.604872703552246, Test Acc: 1.090000033378601, Time: 72.50628709793091 s\n",
      "Epoch: 1, Train Loss: 4.213759899139404, Train Acc:6.668526649475098, Test Loss: 4.604650974273682, Test Acc: 1.5399999618530273, Time: 36.012362241744995 s\n",
      "Epoch: 2, Train Loss: 4.4450554847717285, Train Acc:6.310211658477783, Test Loss: 4.604489326477051, Test Acc: 1.5199999809265137, Time: 36.2438428401947 s\n",
      "Epoch: 3, Train Loss: 4.720391750335693, Train Acc:6.385522842407227, Test Loss: 4.604438781738281, Test Acc: 1.0099999904632568, Time: 45.52060055732727 s\n"
     ]
    }
   ],
   "source": [
    "print(\"starting training\")\n",
    "time_record = ''\n",
    "for epoch in range(epochs):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    batches = 0\n",
    "    while not ((batches + task_train_size + meta_train_size) >= train_size / batch_size):\n",
    "        # get the weights of the initial model that will do the meta learning\n",
    "        meta_model_weights = model.get_weights()\n",
    "\n",
    "        # train on the task (one epoch)\n",
    "        task_batches = 0\n",
    "        for images, labels in train_gen:\n",
    "            batches += 1\n",
    "            train_step(images, labels)\n",
    "            task_batches += 1\n",
    "            if task_batches >= task_train_size:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                break\n",
    "\n",
    "        # test on the validation set the improvement achieved on one task for the meta learning\n",
    "        meta_batches = 0\n",
    "        sum_gradients = np.zeros_like(model.trainable_variables)\n",
    "        for images, labels in train_gen:\n",
    "            batches += 1\n",
    "            gradients = meta_step(images, labels)\n",
    "            gradients = np.array([np.array(x) for x in gradients])\n",
    "            sum_gradients = sum_gradients + gradients\n",
    "            meta_batches += 1\n",
    "            if meta_batches >= meta_train_size:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                break\n",
    "\n",
    "        # set weights of the model to the weights of the original model\n",
    "        model.set_weights(meta_model_weights)                        \n",
    "\n",
    "        # update the weights of the meta learning model using the loss obtained from testing\n",
    "        meta_optimizer.apply_gradients(zip(sum_gradients, model.trainable_variables))\n",
    "            \n",
    "\n",
    "    # get the weights of the initial model that will do the meta learning\n",
    "    meta_model_weights = model.get_weights()\n",
    "\n",
    "    # train on the task (one epoch)\n",
    "    task_batches = 0\n",
    "    for images, labels in train_gen:\n",
    "        batches += 1\n",
    "        train_step(images, labels)\n",
    "        task_batches += 1\n",
    "        if task_batches >= task_train_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    # test the newly trained model on the training set\n",
    "    batches = 0\n",
    "    all_predictions = np.array([]).reshape(0, n_classes)\n",
    "    all_labels = np.array([]).reshape(0, n_classes)\n",
    "    for test_images, test_labels in test_gen:\n",
    "        test_predictions = test_step(test_images, test_labels)\n",
    "        all_predictions = np.vstack((all_predictions, test_predictions))\n",
    "        all_labels = np.vstack((all_labels, tf.one_hot(test_labels, n_classes)))\n",
    "        batches += 1\n",
    "        if batches >= test_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "    \n",
    "    # set weights of the model to the weights of the original model\n",
    "    model.set_weights(meta_model_weights)                        \n",
    "\n",
    "    time_finish = time.time()\n",
    "    end_time = (time_finish-time_start)\n",
    "    time_record = time_record + '{:.3f} s \\n'.format(end_time)\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}, Time: {} s'.format(\n",
    "               epoch,\n",
    "               train_loss.result(),\n",
    "               train_accuracy.result()*100,\n",
    "               test_loss.result(),\n",
    "               test_accuracy.result()*100,\n",
    "               end_time))\n",
    "\n",
    "        if (test_loss.result() < min_loss):    \n",
    "            if not os.path.exists(models_directory):\n",
    "                os.makedirs(models_directory)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(models_directory + \"best{}.h5\".format(identifier))\n",
    "            min_loss = test_loss.result()\n",
    "            min_loss_acc = test_accuracy.result()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "            #tf.summary.image('Confusion Matrix', image, step=epoch)\n",
    "            train_loss.reset_states()           \n",
    "            train_accuracy.reset_states()           \n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            test_loss.reset_states()           \n",
    "            test_accuracy.reset_states()   \n",
    "            # save confusion matrix\n",
    "            con_mat = tf.math.confusion_matrix(\n",
    "                labels=np.argmax(all_labels, axis=1), \n",
    "                predictions=np.argmax(all_predictions, axis=1),\n",
    "                num_classes=n_classes).numpy()\n",
    "            con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "            con_mat_df = pd.DataFrame(con_mat_norm,\n",
    "                                 index = classes, \n",
    "                                 columns = classes)\n",
    "            figure = plt.figure(figsize=(8, 8))\n",
    "            sns.heatmap(con_mat_df, annot=False,cmap=plt.cm.Blues)\n",
    "            plt.tight_layout()\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            plt.close(figure)\n",
    "            buf.seek(0)\n",
    "            image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "            image = tf.expand_dims(image, 0)\n",
    "            tf.summary.image('Confusion Matrix', image, step=epoch)\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        break\n",
    "\n",
    "with open(os.path.join('results/', identifier), \"w\") as file1:\n",
    "    file1.write(time_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
