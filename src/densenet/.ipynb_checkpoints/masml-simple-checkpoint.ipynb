{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from densenet import densenet_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From <ipython-input-2-0dcb167e88d7>:14: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# data\n",
    "rotation_range = 20\n",
    "width_shift_range = 0.2\n",
    "height_shift_range = 0.2\n",
    "horizontal_flip = True\n",
    "vertical_flip = True\n",
    "shear_range = 0\n",
    "zoom_range = 0.5\n",
    "size = (32,32)\n",
    "\n",
    "# model\n",
    "nb_filter = 64\n",
    "growth_rate = 16\n",
    "nb_layers = [6, 12, 24, 16]\n",
    "reduction = 0.5\n",
    "\n",
    "# training\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "max_patience = 100\n",
    "batch_size = 512\n",
    "\n",
    "# log\n",
    "log_freq = 1\n",
    "models_directory = 'results/models/'\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "identifier = \"{}-growth-{}-densenet\".format(\n",
    "    '-'.join([str(i) for i in nb_layers]),\n",
    "    growth_rate) + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tf.data.Dataset\n",
    "ds_train = tfds.load('cifar100', split='train', shuffle_files=True, batch_size=-1)\n",
    "train_np_ds = tfds.as_numpy(ds_train)\n",
    "ds_test = tfds.load('cifar100', split='test', shuffle_files=False, batch_size=-1)\n",
    "test_np_ds = tfds.as_numpy(ds_test)\n",
    "\n",
    "x_train, y_train = train_np_ds[\"image\"], train_np_ds[\"label\"]\n",
    "x_test, y_test = test_np_ds[\"image\"], test_np_ds[\"label\"]\n",
    "\n",
    "# shuffle the meta train images maintaining the same label order\n",
    "index_sets = [np.argwhere(i==y_train) for i in np.unique(y_train)]\n",
    "x_meta_train = np.copy(x_train)\n",
    "for class_indexes in index_sets:\n",
    "    shuffled_class_indexes = np.copy(class_indexes)\n",
    "    np.random.shuffle(shuffled_class_indexes)\n",
    "    for i in range(len(class_indexes)):\n",
    "        x_meta_train[class_indexes[i]] = x_train[shuffled_class_indexes[i]]\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "train_epochs = 1\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "\n",
    "info = tfds.builder('cifar100').info\n",
    "n_classes = info.features['label'].num_classes\n",
    "img_shape = info.features['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=rotation_range,\n",
    "    width_shift_range=width_shift_range,\n",
    "    height_shift_range=height_shift_range,\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    vertical_flip = vertical_flip,\n",
    "    shear_range=shear_range,\n",
    "    zoom_range=zoom_range,\n",
    "    fill_mode='constant',\n",
    "    cval=0,\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    ")\n",
    "\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# create data generators\n",
    "train_gen =  datagen.flow(x_train, y_train, batch_size=batch_size, seed=42)\n",
    "meta_train_gen =  datagen.flow(x_meta_train, y_train, batch_size=batch_size, seed=42)\n",
    "test_gen = test_datagen.flow(x_test, y_test , batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "batches = 0\n",
    "for images, labels in train_gen:\n",
    "    batches += 1\n",
    "    all_labels = np.append(all_labels, labels)    \n",
    "    if batches >= train_size / batch_size:\n",
    "        # we need to break the loop by hand because\n",
    "        # the generator loops indefinitely\n",
    "        break\n",
    "all_meta_labels = []\n",
    "batches = 0\n",
    "for images, labels in meta_train_gen:\n",
    "    batches += 1\n",
    "    all_meta_labels = np.append(all_meta_labels, labels)    \n",
    "    if batches >= train_size / batch_size:\n",
    "        # we need to break the loop by hand because\n",
    "        # the generator loops indefinitely\n",
    "        break\n",
    "print(all_labels == all_meta_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_gen:\n",
    "    plt.imshow(images[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet_model(classes=n_classes, nb_filter=nb_filter, shape=img_shape, growth_rate=growth_rate, nb_layers=nb_layers, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "meta_optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def meta_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary writers\n",
    "train_summary_writer = tf.summary.create_file_writer('results/summaries/train/' + identifier)\n",
    "test_summary_writer = tf.summary.create_file_writer('results/summaries/test/' + identifier)\n",
    "\n",
    "min_loss = 100\n",
    "min_loss_acc = 0\n",
    "patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "train epoch: 0\n",
      "Epoch: 0, Train Loss: 4.109673976898193, Train Acc:7.935000419616699, Test Loss: 4.633959770202637, Test Acc: 1.0299999713897705, Time: 127.15767025947571 s\n",
      "train epoch: 0\n",
      "Epoch: 1, Train Loss: 4.218905925750732, Train Acc:8.22599983215332, Test Loss: 4.613393306732178, Test Acc: 1.1299999952316284, Time: 82.045175075531 s\n",
      "train epoch: 0\n",
      "Epoch: 2, Train Loss: 4.0316162109375, Train Acc:9.972000122070312, Test Loss: 4.640814304351807, Test Acc: 1.3000000715255737, Time: 82.626797914505 s\n",
      "train epoch: 0\n",
      "Epoch: 3, Train Loss: 3.8719756603240967, Train Acc:12.029000282287598, Test Loss: 4.617884635925293, Test Acc: 1.5900001525878906, Time: 82.8013596534729 s\n",
      "train epoch: 0\n",
      "Epoch: 4, Train Loss: 3.9515247344970703, Train Acc:12.545999526977539, Test Loss: 4.625624656677246, Test Acc: 1.5299999713897705, Time: 83.19845032691956 s\n",
      "train epoch: 0\n",
      "Epoch: 5, Train Loss: 3.73685622215271, Train Acc:14.62700080871582, Test Loss: 4.6166181564331055, Test Acc: 1.4100000858306885, Time: 81.40743517875671 s\n",
      "train epoch: 0\n",
      "Epoch: 6, Train Loss: 3.5594582557678223, Train Acc:16.645999908447266, Test Loss: 4.61613130569458, Test Acc: 1.8600000143051147, Time: 82.45006918907166 s\n",
      "train epoch: 0\n",
      "Epoch: 7, Train Loss: 3.6629059314727783, Train Acc:15.899999618530273, Test Loss: 4.58842658996582, Test Acc: 1.1100000143051147, Time: 83.7027280330658 s\n",
      "train epoch: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"starting training\")\n",
    "time_record = ''\n",
    "for epoch in range(epochs):\n",
    "    time_start = time.time()\n",
    "\n",
    "    for train_epoch in range(train_epochs):\n",
    "        print(\"train epoch: \" + str(train_epoch))\n",
    "        batches = 0\n",
    "        while batches < train_size / batch_size:\n",
    "\n",
    "            batches += 1\n",
    "\n",
    "            # get the weights of the initial model that will do the meta learning\n",
    "            meta_model_weights = model.get_weights()\n",
    "\n",
    "            # train on the task (one batch)\n",
    "            images, labels = train_gen.next()\n",
    "            train_step(images, labels)\n",
    "\n",
    "            # test on the validation set the improvement achieved on one task for the meta learning\n",
    "            images, labels = meta_train_gen.next()\n",
    "            gradients = meta_step(images, labels)\n",
    "\n",
    "            # set weights of the model to the weights of the original model\n",
    "            model.set_weights(meta_model_weights)                        \n",
    "\n",
    "            # update the weights of the meta learning model using the loss obtained from testing\n",
    "            meta_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # get the weights of the initial model that will do the meta learning\n",
    "    meta_model_weights = model.get_weights()\n",
    "\n",
    "    # train on the task (one epoch)\n",
    "    batches = 0\n",
    "    for images, labels in train_gen:\n",
    "        batches += 1\n",
    "        train_step(images, labels)\n",
    "        if batches >= train_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    # test the newly trained model on the training set\n",
    "    batches = 0\n",
    "    for test_images, test_labels in test_gen:\n",
    "        test_step(test_images, test_labels)\n",
    "        batches += 1\n",
    "        if batches >= test_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "    \n",
    "    # set weights of the model to the weights of the original model\n",
    "    model.set_weights(meta_model_weights)                        \n",
    "\n",
    "    time_finish = time.time()\n",
    "    end_time = (time_finish-time_start)\n",
    "    time_record = time_record + '{:.3f} s \\n'.format(end_time)\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}, Time: {} s'.format(\n",
    "               epoch,\n",
    "               train_loss.result(),\n",
    "               train_accuracy.result()*100,\n",
    "               test_loss.result(),\n",
    "               test_accuracy.result()*100,\n",
    "               end_time))\n",
    "\n",
    "        if (test_loss.result() < min_loss):    \n",
    "            if not os.path.exists(models_directory):\n",
    "                os.makedirs(models_directory)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(models_directory + \"best{}.h5\".format(identifier))\n",
    "            min_loss = test_loss.result()\n",
    "            min_loss_acc = test_accuracy.result()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "            train_loss.reset_states()           \n",
    "            train_accuracy.reset_states()           \n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            test_loss.reset_states()           \n",
    "            test_accuracy.reset_states()   \n",
    "\n",
    "    if patience >= max_patience:\n",
    "        break\n",
    "\n",
    "with open(os.path.join('results/', identifier), \"w\") as file1:\n",
    "    file1.write(time_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
