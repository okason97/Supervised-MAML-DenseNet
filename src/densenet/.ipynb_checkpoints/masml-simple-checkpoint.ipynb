{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from densenet import densenet_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From <ipython-input-2-0dcb167e88d7>:14: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# data\n",
    "rotation_range = 20\n",
    "width_shift_range = 0.2\n",
    "height_shift_range = 0.2\n",
    "horizontal_flip = True\n",
    "vertical_flip = True\n",
    "shear_range = 0\n",
    "zoom_range = 0.5\n",
    "size = (32,32)\n",
    "\n",
    "# model\n",
    "nb_filter = 64\n",
    "growth_rate = 64\n",
    "nb_layers = [6, 12, 16]\n",
    "reduction = 0.5\n",
    "\n",
    "# training\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "max_patience = 100\n",
    "batch_size = 256\n",
    "train_epochs = 3\n",
    "\n",
    "# log\n",
    "log_freq = 1\n",
    "models_directory = 'results/models/'\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "identifier = \"{}-growth-{}-densenet\".format(\n",
    "    '-'.join([str(i) for i in nb_layers]),\n",
    "    growth_rate) + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tf.data.Dataset\n",
    "ds_train = tfds.load('cifar100', split='train', shuffle_files=True, batch_size=-1)\n",
    "train_np_ds = tfds.as_numpy(ds_train)\n",
    "ds_test = tfds.load('cifar100', split='test', shuffle_files=False, batch_size=-1)\n",
    "test_np_ds = tfds.as_numpy(ds_test)\n",
    "\n",
    "x_train, y_train = train_np_ds[\"image\"], train_np_ds[\"label\"]\n",
    "x_test, y_test = test_np_ds[\"image\"], test_np_ds[\"label\"]\n",
    "\n",
    "# shuffle the meta train images maintaining the same label order\n",
    "index_sets = [np.argwhere(i==y_train) for i in np.unique(y_train)]\n",
    "x_meta_train = np.copy(x_train)\n",
    "for class_indexes in index_sets:\n",
    "    shuffled_class_indexes = np.copy(class_indexes)\n",
    "    np.random.shuffle(shuffled_class_indexes)\n",
    "    for i in range(len(class_indexes)):\n",
    "        x_meta_train[class_indexes[i]] = x_train[shuffled_class_indexes[i]]\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "\n",
    "info = tfds.builder('cifar100').info\n",
    "n_classes = info.features['label'].num_classes\n",
    "img_shape = info.features['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=rotation_range,\n",
    "    width_shift_range=width_shift_range,\n",
    "    height_shift_range=height_shift_range,\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    vertical_flip = vertical_flip,\n",
    "    shear_range=shear_range,\n",
    "    zoom_range=zoom_range,\n",
    "    fill_mode='constant',\n",
    "    cval=0,\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    ")\n",
    "\n",
    "test_datagen.fit(x_train)\n",
    "\n",
    "# create data generators\n",
    "train_gen =  datagen.flow(x_train, y_train, batch_size=batch_size, seed=42)\n",
    "meta_train_gen =  datagen.flow(x_meta_train, y_train, batch_size=batch_size, seed=42)\n",
    "test_gen = test_datagen.flow(x_test, y_test , batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "batches = 0\n",
    "for images, labels in train_gen:\n",
    "    batches += 1\n",
    "    all_labels = np.append(all_labels, labels)    \n",
    "    if batches >= train_size / batch_size:\n",
    "        # we need to break the loop by hand because\n",
    "        # the generator loops indefinitely\n",
    "        break\n",
    "all_meta_labels = []\n",
    "batches = 0\n",
    "for images, labels in meta_train_gen:\n",
    "    batches += 1\n",
    "    all_meta_labels = np.append(all_meta_labels, labels)    \n",
    "    if batches >= train_size / batch_size:\n",
    "        # we need to break the loop by hand because\n",
    "        # the generator loops indefinitely\n",
    "        break\n",
    "print(all_labels == all_meta_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_gen:\n",
    "    plt.imshow(images[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet_model(classes=n_classes, nb_filter=nb_filter, shape=img_shape, growth_rate=growth_rate, nb_layers=nb_layers, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "meta_optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def meta_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary writers\n",
    "train_summary_writer = tf.summary.create_file_writer('results/summaries/train/' + identifier)\n",
    "test_summary_writer = tf.summary.create_file_writer('results/summaries/test/' + identifier)\n",
    "\n",
    "min_loss = 100\n",
    "min_loss_acc = 0\n",
    "patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "train epoch: 0\n",
      "Epoch: 0, Train Loss: 4.788522720336914, Train Acc:7.517000198364258, Test Loss: 4.041277885437012, Test Acc: 8.399999618530273, Time: 149.86185264587402 s\n",
      "train epoch: 0\n",
      "Epoch: 1, Train Loss: 4.438211441040039, Train Acc:10.767999649047852, Test Loss: 3.9518508911132812, Test Acc: 9.579999923706055, Time: 120.83740496635437 s\n",
      "train epoch: 0\n",
      "Epoch: 2, Train Loss: 4.296205043792725, Train Acc:12.967999458312988, Test Loss: 3.6491916179656982, Test Acc: 14.15999984741211, Time: 121.91885280609131 s\n",
      "train epoch: 0\n",
      "Epoch: 3, Train Loss: 4.10151481628418, Train Acc:14.9660005569458, Test Loss: 3.480398178100586, Test Acc: 17.03999900817871, Time: 121.99748015403748 s\n",
      "train epoch: 0\n",
      "Epoch: 4, Train Loss: 3.702087163925171, Train Acc:18.920000076293945, Test Loss: 3.432218074798584, Test Acc: 17.560001373291016, Time: 121.89646482467651 s\n",
      "train epoch: 0\n",
      "Epoch: 5, Train Loss: 3.4399161338806152, Train Acc:21.277000427246094, Test Loss: 3.470823287963867, Test Acc: 17.059999465942383, Time: 120.49592304229736 s\n",
      "train epoch: 0\n",
      "Epoch: 6, Train Loss: 3.2719404697418213, Train Acc:22.969999313354492, Test Loss: 3.3461825847625732, Test Acc: 18.739999771118164, Time: 123.15103697776794 s\n",
      "train epoch: 0\n",
      "Epoch: 7, Train Loss: 3.5926146507263184, Train Acc:21.47800064086914, Test Loss: 3.5510990619659424, Test Acc: 16.440000534057617, Time: 122.43538928031921 s\n",
      "train epoch: 0\n",
      "Epoch: 8, Train Loss: 2.9797587394714355, Train Acc:27.810001373291016, Test Loss: 3.42162823677063, Test Acc: 18.309999465942383, Time: 123.08252501487732 s\n",
      "train epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-35cb20bcab94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# set weights of the model to the weights of the original model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_model_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# update the weights of the meta learning model using the loss obtained from testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mspecifications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \"\"\"\n\u001b[0;32m-> 1510\u001b[0;31m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m     \u001b[0mexpected_num_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \"\"\"\n\u001b[0;32m--> 500\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_undeduplicated_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_dedup_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0;31m# Track the Variable's identity to avoid __eq__ issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/object_identity.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/object_identity.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# id(weakref.ref(a)) == id(weakref.ref(a)) and weakref.ref(a) is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# weakref.ref(a) in _WeakObjectIdentityWrapper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"starting training\")\n",
    "time_record = ''\n",
    "for epoch in range(epochs):\n",
    "    time_start = time.time()\n",
    "\n",
    "    for train_epoch in range(train_epochs):\n",
    "        print(\"train epoch: \" + str(train_epoch))\n",
    "        batches = 0\n",
    "        while batches < train_size / batch_size:\n",
    "\n",
    "            batches += 1\n",
    "\n",
    "            # get the weights of the initial model that will do the meta learning\n",
    "            meta_model_weights = model.get_weights()\n",
    "\n",
    "            # train on the task (one batch)\n",
    "            images, labels = train_gen.next()\n",
    "            train_step(images, labels)\n",
    "\n",
    "            # test on the validation set the improvement achieved on one task for the meta learning\n",
    "            images, labels = meta_train_gen.next()\n",
    "            gradients = meta_step(images, labels)\n",
    "\n",
    "            # set weights of the model to the weights of the original model\n",
    "            model.set_weights(meta_model_weights)                        \n",
    "\n",
    "            # update the weights of the meta learning model using the loss obtained from testing\n",
    "            meta_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # get the weights of the initial model that will do the meta learning\n",
    "    meta_model_weights = model.get_weights()\n",
    "\n",
    "    # train on the task (one epoch)\n",
    "    batches = 0\n",
    "    for images, labels in train_gen:\n",
    "        batches += 1\n",
    "        train_step(images, labels)\n",
    "        if batches >= train_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    # test the newly trained model on the training set\n",
    "    batches = 0\n",
    "    for test_images, test_labels in test_gen:\n",
    "        test_step(test_images, test_labels)\n",
    "        batches += 1\n",
    "        if batches >= test_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "    \n",
    "    # set weights of the model to the weights of the original model\n",
    "    model.set_weights(meta_model_weights)                        \n",
    "\n",
    "    time_finish = time.time()\n",
    "    end_time = (time_finish-time_start)\n",
    "    time_record = time_record + '{:.3f} s \\n'.format(end_time)\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}, Time: {} s'.format(\n",
    "               epoch,\n",
    "               train_loss.result(),\n",
    "               train_accuracy.result()*100,\n",
    "               test_loss.result(),\n",
    "               test_accuracy.result()*100,\n",
    "               end_time))\n",
    "\n",
    "        if (test_loss.result() < min_loss):    \n",
    "            if not os.path.exists(models_directory):\n",
    "                os.makedirs(models_directory)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(models_directory + \"best{}.h5\".format(identifier))\n",
    "            min_loss = test_loss.result()\n",
    "            min_loss_acc = test_accuracy.result()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "            train_loss.reset_states()           \n",
    "            train_accuracy.reset_states()           \n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            test_loss.reset_states()           \n",
    "            test_accuracy.reset_states()   \n",
    "\n",
    "    if patience >= max_patience:\n",
    "        break\n",
    "\n",
    "with open(os.path.join('results/', identifier), \"w\") as file1:\n",
    "    file1.write(time_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
