{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from densenet import densenet_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From <ipython-input-2-0dcb167e88d7>:14: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# data\n",
    "rotation_range = 20\n",
    "width_shift_range = 0.2\n",
    "height_shift_range = 0.2\n",
    "horizontal_flip = True\n",
    "vertical_flip = True\n",
    "shear_range = 0\n",
    "zoom_range = 0.5\n",
    "size = (32,32)\n",
    "\n",
    "# model\n",
    "nb_filter = 64\n",
    "growth_rate = 16\n",
    "nb_layers = [6, 12, 24, 16]\n",
    "reduction = 0.5\n",
    "\n",
    "# training\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "max_patience = 100\n",
    "batch_size = 512\n",
    "\n",
    "# log\n",
    "log_freq = 1\n",
    "models_directory = 'results/models/'\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "identifier = \"{}-growth-{}-densenet\".format(\n",
    "    '-'.join([str(i) for i in nb_layers]),\n",
    "    growth_rate) + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tf.data.Dataset\n",
    "ds_train = tfds.load('cifar100', split='train', shuffle_files=True, batch_size=-1)\n",
    "train_np_ds = tfds.as_numpy(ds_train)\n",
    "ds_test = tfds.load('cifar100', split='test', shuffle_files=False, batch_size=-1)\n",
    "test_np_ds = tfds.as_numpy(ds_test)\n",
    "\n",
    "x_train, y_train = train_np_ds[\"image\"], train_np_ds[\"label\"]\n",
    "x_test, y_test = test_np_ds[\"image\"], test_np_ds[\"label\"]\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "task_train_size = 1\n",
    "meta_train_size = 1\n",
    "train_size = x_train.shape[0]\n",
    "test_size = x_test.shape[0]\n",
    "\n",
    "info = tfds.builder('cifar100').info\n",
    "n_classes = info.features['label'].num_classes\n",
    "img_shape = info.features['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=rotation_range,\n",
    "    width_shift_range=width_shift_range,\n",
    "    height_shift_range=height_shift_range,\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    vertical_flip = vertical_flip,\n",
    "    shear_range=shear_range,\n",
    "    zoom_range=zoom_range,\n",
    "    fill_mode='constant',\n",
    "    cval=0,\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    ")\n",
    "\n",
    "test_datagen.fit(x_test)\n",
    "\n",
    "# create data generators\n",
    "train_gen =  datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "test_gen = test_datagen.flow(x_test, y_test , batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " [[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " [[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " [[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]\n",
      "\n",
      " [[-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  ...\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]\n",
      "  [-1.3062079 -1.3746256 -1.2926631]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_gen:\n",
    "    print(images[0])\n",
    "    plt.imshow(images[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet_model(classes=n_classes, nb_filter=nb_filter, shape=img_shape, growth_rate=growth_rate, nb_layers=nb_layers, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "meta_optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def meta_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary writers\n",
    "train_summary_writer = tf.summary.create_file_writer('results/summaries/train/' + identifier)\n",
    "test_summary_writer = tf.summary.create_file_writer('results/summaries/test/' + identifier)\n",
    "\n",
    "min_loss = 100\n",
    "min_loss_acc = 0\n",
    "patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Epoch: 0, Train Loss: 4.4445271492004395, Train Acc:4.2236328125, Test Loss: 4.60487174987793, Test Acc: 1.0, Time: 62.96587944030762 s\n",
      "Epoch: 1, Train Loss: 4.225707530975342, Train Acc:6.881147384643555, Test Loss: 4.60474967956543, Test Acc: 1.0299999713897705, Time: 44.113823890686035 s\n",
      "Epoch: 2, Train Loss: 4.412186622619629, Train Acc:6.110655784606934, Test Loss: 4.604813098907471, Test Acc: 1.1399999856948853, Time: 35.711406230926514 s\n",
      "Epoch: 3, Train Loss: 4.731334686279297, Train Acc:5.073770523071289, Test Loss: 4.604809284210205, Test Acc: 1.0299999713897705, Time: 35.30387544631958 s\n",
      "Epoch: 4, Train Loss: 4.845445156097412, Train Acc:5.0901641845703125, Test Loss: 4.604743003845215, Test Acc: 1.0, Time: 35.33125424385071 s\n",
      "Epoch: 5, Train Loss: 4.702964782714844, Train Acc:5.8401641845703125, Test Loss: 4.604619979858398, Test Acc: 1.0, Time: 35.296931982040405 s\n",
      "Epoch: 6, Train Loss: 4.720049858093262, Train Acc:6.020492076873779, Test Loss: 4.604626178741455, Test Acc: 1.0, Time: 35.12900376319885 s\n",
      "Epoch: 7, Train Loss: 4.594274997711182, Train Acc:7.176229476928711, Test Loss: 4.604615688323975, Test Acc: 1.0, Time: 35.19381546974182 s\n",
      "Epoch: 8, Train Loss: 4.731427192687988, Train Acc:7.262294769287109, Test Loss: 4.60452938079834, Test Acc: 1.0, Time: 35.087008476257324 s\n",
      "Epoch: 9, Train Loss: 4.424032688140869, Train Acc:9.733607292175293, Test Loss: 4.604389190673828, Test Acc: 1.0, Time: 35.30344009399414 s\n",
      "Epoch: 10, Train Loss: 4.389761447906494, Train Acc:9.610655784606934, Test Loss: 4.604280948638916, Test Acc: 1.0, Time: 35.26984620094299 s\n",
      "Epoch: 11, Train Loss: 4.529430389404297, Train Acc:9.25, Test Loss: 4.604079246520996, Test Acc: 1.0, Time: 35.24558162689209 s\n",
      "Epoch: 12, Train Loss: 4.414821624755859, Train Acc:10.286885261535645, Test Loss: 4.604035377502441, Test Acc: 1.0, Time: 35.284313917160034 s\n",
      "Epoch: 13, Train Loss: 4.277178764343262, Train Acc:11.311474800109863, Test Loss: 4.60402250289917, Test Acc: 1.0, Time: 35.22998547554016 s\n",
      "Epoch: 14, Train Loss: 4.092077732086182, Train Acc:12.897541046142578, Test Loss: 4.604007720947266, Test Acc: 1.0, Time: 35.38970184326172 s\n",
      "Epoch: 15, Train Loss: 4.019951343536377, Train Acc:13.852458953857422, Test Loss: 4.603912830352783, Test Acc: 1.159999966621399, Time: 35.23132038116455 s\n",
      "Epoch: 16, Train Loss: 3.9473230838775635, Train Acc:14.647540092468262, Test Loss: 4.603969573974609, Test Acc: 0.9900000095367432, Time: 35.24874973297119 s\n",
      "Epoch: 17, Train Loss: 3.8717806339263916, Train Acc:15.069672584533691, Test Loss: 4.60404634475708, Test Acc: 1.0, Time: 35.19828748703003 s\n",
      "Epoch: 18, Train Loss: 3.9962427616119385, Train Acc:14.229507446289062, Test Loss: 4.6039228439331055, Test Acc: 1.2200000286102295, Time: 35.37663388252258 s\n",
      "Epoch: 19, Train Loss: 3.9934701919555664, Train Acc:14.459016799926758, Test Loss: 4.604029655456543, Test Acc: 1.090000033378601, Time: 35.40521049499512 s\n",
      "Epoch: 20, Train Loss: 4.024929523468018, Train Acc:14.680327415466309, Test Loss: 4.6038594245910645, Test Acc: 1.0299999713897705, Time: 35.26932907104492 s\n",
      "Epoch: 21, Train Loss: 3.990062713623047, Train Acc:15.774589538574219, Test Loss: 4.604031562805176, Test Acc: 1.4000000953674316, Time: 35.35942029953003 s\n",
      "Epoch: 22, Train Loss: 3.884227752685547, Train Acc:16.495901107788086, Test Loss: 4.604009628295898, Test Acc: 1.2400000095367432, Time: 35.2683379650116 s\n",
      "Epoch: 23, Train Loss: 3.8292596340179443, Train Acc:17.286884307861328, Test Loss: 4.6041059494018555, Test Acc: 1.4800000190734863, Time: 35.21938443183899 s\n",
      "Epoch: 24, Train Loss: 3.849367141723633, Train Acc:18.004098892211914, Test Loss: 4.603996753692627, Test Acc: 1.2100000381469727, Time: 35.26142239570618 s\n",
      "Epoch: 25, Train Loss: 3.759345293045044, Train Acc:19.76229476928711, Test Loss: 4.6037917137146, Test Acc: 1.0800000429153442, Time: 35.33866357803345 s\n",
      "Epoch: 26, Train Loss: 3.6767404079437256, Train Acc:19.852458953857422, Test Loss: 4.60385799407959, Test Acc: 1.0199999809265137, Time: 35.209691762924194 s\n",
      "Epoch: 27, Train Loss: 3.543872594833374, Train Acc:20.758195877075195, Test Loss: 4.603899002075195, Test Acc: 1.1200000047683716, Time: 35.28753852844238 s\n",
      "Epoch: 28, Train Loss: 3.541672945022583, Train Acc:20.655736923217773, Test Loss: 4.603548526763916, Test Acc: 1.25, Time: 35.74777889251709 s\n",
      "Epoch: 29, Train Loss: 3.4439306259155273, Train Acc:21.348360061645508, Test Loss: 4.603810787200928, Test Acc: 1.1200000047683716, Time: 35.65566921234131 s\n",
      "Epoch: 30, Train Loss: 3.366645574569702, Train Acc:22.532787322998047, Test Loss: 4.603269577026367, Test Acc: 1.059999942779541, Time: 35.285287857055664 s\n"
     ]
    }
   ],
   "source": [
    "print(\"starting training\")\n",
    "time_record = ''\n",
    "for epoch in range(epochs):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    batches = 0\n",
    "    while not ((batches + task_train_size + meta_train_size) >= train_size / batch_size):\n",
    "        # get the weights of the initial model that will do the meta learning\n",
    "        meta_model_weights = model.get_weights()\n",
    "\n",
    "        # train on the task (one epoch)\n",
    "        task_batches = 0\n",
    "        for images, labels in train_gen:\n",
    "            batches += 1\n",
    "            train_step(images, labels)\n",
    "            task_batches += 1\n",
    "            if task_batches >= task_train_size:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                break\n",
    "\n",
    "        # test on the validation set the improvement achieved on one task for the meta learning\n",
    "        meta_batches = 0\n",
    "        sum_gradients = np.zeros_like(model.trainable_variables)\n",
    "        for images, labels in train_gen:\n",
    "            batches += 1\n",
    "            gradients = meta_step(images, labels)\n",
    "            gradients = np.array([np.array(x) for x in gradients])\n",
    "            sum_gradients = sum_gradients + gradients\n",
    "            meta_batches += 1\n",
    "            if meta_batches >= meta_train_size:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                break\n",
    "\n",
    "        # set weights of the model to the weights of the original model\n",
    "        model.set_weights(meta_model_weights)                        \n",
    "\n",
    "        # update the weights of the meta learning model using the loss obtained from testing\n",
    "        meta_optimizer.apply_gradients(zip(sum_gradients, model.trainable_variables))\n",
    "            \n",
    "\n",
    "    # get the weights of the initial model that will do the meta learning\n",
    "    meta_model_weights = model.get_weights()\n",
    "\n",
    "    # train on the task (one epoch)\n",
    "    task_batches = 0\n",
    "    for images, labels in train_gen:\n",
    "        batches += 1\n",
    "        train_step(images, labels)\n",
    "        task_batches += 1\n",
    "        if task_batches >= task_train_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    # test the newly trained model on the training set\n",
    "    batches = 0\n",
    "    all_predictions = np.array([]).reshape(0, n_classes)\n",
    "    all_labels = np.array([]).reshape(0, n_classes)\n",
    "    for test_images, test_labels in test_gen:\n",
    "        test_predictions = test_step(test_images, test_labels)\n",
    "        all_predictions = np.vstack((all_predictions, test_predictions))\n",
    "        all_labels = np.vstack((all_labels, tf.one_hot(test_labels, n_classes)))\n",
    "        batches += 1\n",
    "        if batches >= test_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "    \n",
    "    # set weights of the model to the weights of the original model\n",
    "    model.set_weights(meta_model_weights)                        \n",
    "\n",
    "    time_finish = time.time()\n",
    "    end_time = (time_finish-time_start)\n",
    "    time_record = time_record + '{:.3f} s \\n'.format(end_time)\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}, Time: {} s'.format(\n",
    "               epoch,\n",
    "               train_loss.result(),\n",
    "               train_accuracy.result()*100,\n",
    "               test_loss.result(),\n",
    "               test_accuracy.result()*100,\n",
    "               end_time))\n",
    "\n",
    "        if (test_loss.result() < min_loss):    \n",
    "            if not os.path.exists(models_directory):\n",
    "                os.makedirs(models_directory)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(models_directory + \"best{}.h5\".format(identifier))\n",
    "            min_loss = test_loss.result()\n",
    "            min_loss_acc = test_accuracy.result()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "            #tf.summary.image('Confusion Matrix', image, step=epoch)\n",
    "            train_loss.reset_states()           \n",
    "            train_accuracy.reset_states()           \n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            test_loss.reset_states()           \n",
    "            test_accuracy.reset_states()   \n",
    "            # save confusion matrix\n",
    "            con_mat = tf.math.confusion_matrix(\n",
    "                labels=np.argmax(all_labels, axis=1), \n",
    "                predictions=np.argmax(all_predictions, axis=1),\n",
    "                num_classes=n_classes).numpy()\n",
    "            con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "            con_mat_df = pd.DataFrame(con_mat_norm,\n",
    "                                 index = classes, \n",
    "                                 columns = classes)\n",
    "            figure = plt.figure(figsize=(8, 8))\n",
    "            sns.heatmap(con_mat_df, annot=False,cmap=plt.cm.Blues)\n",
    "            plt.tight_layout()\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            plt.close(figure)\n",
    "            buf.seek(0)\n",
    "            image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "            image = tf.expand_dims(image, 0)\n",
    "            tf.summary.image('Confusion Matrix', image, step=epoch)\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        break\n",
    "\n",
    "with open(os.path.join('results/', identifier), \"w\") as file1:\n",
    "    file1.write(time_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
